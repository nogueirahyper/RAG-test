# ============================================================
# RAG COMPLETO (FAISS + SentenceTransformer + FLAN-T5)
# ============================================================

from pathlib import Path
import json
from typing import List

# Embeddings
from sentence_transformers import SentenceTransformer
import numpy as np

# Vector DB (FAISS)
import faiss

# LangChain Document (opcional)
from langchain.docstore.document import Document

# Hugging Face (geração)
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline


# ============================
# 3) CONFIGURATION
# ============================

DATA_FILE = Path("Elite_da_tropa.txt")

EMBEDDING_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
GENERATION_MODEL = "google/flan-t5-base"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50


# ============================
# 4) LOAD + CHUNKING
# ============================

def load_texts(file_path: Path) -> List[Document]:
    """Carrega o arquivo e divide em chunks."""
    if not file_path.exists():
        raise FileNotFoundError(f"Arquivo não encontrado: {file_path}")

    text = file_path.read_text(encoding="utf-8")
    docs = []
    start = 0

    while start < len(text):
        chunk = text[start:start + CHUNK_SIZE]

        metadata = {"source": file_path.name, "start": start}
        docs.append(Document(page_content=chunk, metadata=metadata))

        start += CHUNK_SIZE - CHUNK_OVERLAP

    return docs


# ============================
# 5) CREATE EMBEDDINGS + FAISS
# ============================

print("Loading embedding model...")
embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)

print("Loading documents...")
docs = load_texts(DATA_FILE)
texts = [d.page_content for d in docs]

print("Computing embeddings...")
embs = embedder.encode(
    texts,
    show_progress_bar=True,
    convert_to_numpy=True
)

# Criação do índice FAISS
dim = embs.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(embs)

print(f"FAISS index contains {index.ntotal} vectors")

# Salvando índice e metadata
faiss.write_index(index, "faiss_index.bin")

with open("faiss_metadata.json", "w", encoding="utf-8") as f:
    json.dump([d.metadata for d in docs], f, ensure_ascii=False)

print("Index and metadata saved!")


# ============================
# 6) RETRIEVAL FUNCTION
# ============================

def retrieve(query: str, top_k: int = 3):
    """Retorna documentos mais similares no FAISS."""
    q_emb = embedder.encode([query], convert_to_numpy=True)

    distances, indices = index.search(q_emb, top_k)
    results = [docs[i] for i in indices[0]]

    return results


# ============================
# 7) GENERATION MODEL (FLAN-T5)
# ============================

print("Loading generation model...")
tokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL)
model = AutoModelForSeq2SeqLM.from_pretrained(GENERATION_MODEL)

generator = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer
)


# ============================
# 8) ANSWER QUESTION (RAG)
# ============================

def answer_question(query: str, top_k: int = 3):
    retrieved_docs = retrieve(query, top_k=top_k)

    context = "\n\n".join([d.page_content for d in retrieved_docs])

    prompt = (
        f"Use o contexto abaixo para responder a pergunta.\n\n"
        f"Contexto:\n{context}\n\n"
        f"Pergunta: {query}\n"
        f"Resposta:"
    )

    response = generator(prompt, max_length=300)[0]["generated_text"]
    return response


# ============================
# 9) TESTE DO RAG
# ============================

print("\n========== RAG TEST ==========\n")
pergunta = "Quem é Mathias?"
resposta = answer_question(pergunta)
print("Pergunta:", pergunta)
print("\nResposta RAG:")
print(resposta)
